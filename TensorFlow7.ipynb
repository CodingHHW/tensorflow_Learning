{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow卷积神经网络CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:42.756115",
     "start_time": "2017-02-27T13:46:42.750759"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:42.963246",
     "start_time": "2017-02-27T13:46:42.759053"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 10)\n",
      "[ 0.          0.          0.01960784  0.05098039  0.03529412  0.00392157\n",
      "  0.          0.          0.          0.          0.05098039  0.05882353\n",
      "  0.03921569  0.05882353  0.01960784  0.          0.          0.01176471\n",
      "  0.05882353  0.00784314  0.          0.04313726  0.03137255  0.          0.\n",
      "  0.01568628  0.04705882  0.          0.          0.03137255  0.03137255\n",
      "  0.          0.          0.01960784  0.03137255  0.          0.\n",
      "  0.03529412  0.03137255  0.          0.          0.01568628  0.04313726\n",
      "  0.          0.00392157  0.04705882  0.02745098  0.          0.\n",
      "  0.00784314  0.05490196  0.01960784  0.03921569  0.04705882  0.          0.\n",
      "  0.          0.          0.02352941  0.05098039  0.03921569  0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data.astype('float32')\n",
    "y = digits.target\n",
    "y = LabelBinarizer().fit_transform(y)\n",
    "X = X / 255\n",
    "print X.shape\n",
    "print y.shape\n",
    "print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:42.988774",
     "start_time": "2017-02-27T13:46:42.979618"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257, 64) (1257, 10)\n",
      "(540, 64) (540, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着呢，我们定义Weight变量，输入shape，返回变量的参数。其中我们使用tf.truncted_normal产生随机变量来进行初始化:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.003585",
     "start_time": "2017-02-27T13:46:42.994526"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    inital = tf.truncated_normal(shape, stddev=0.1) # 均值为0，方差为0.1的正态分布\n",
    "    return tf.Variable(inital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的定义biase变量，输入shape ,返回变量的一些参数。其中我们使用tf.constant常量函数来进行初始化:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.014080",
     "start_time": "2017-02-27T13:46:43.007843"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义卷积，`tf.nn.conv2d`函数是tensoflow里面的二维的卷积函数，x是图片的所有参数，W是此卷积层的权重，然后定义步长strides=[1,1,1,1]值，strides[0]和strides[3]的两个1是默认值，中间两个1代表padding时在x方向运动一步，y方向运动一步，padding采用的方式是SAME。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.021891",
     "start_time": "2017-02-27T13:46:43.017140"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着定义池化pooling，为了得到更多的图片信息，padding时我们选的是一次一步，也就是strides[1]=strides[2]=1，这样得到的图片尺寸没有变化，而我们希望压缩一下图片也就是参数能少一些从而减小系统的复杂度，因此我们采用pooling来稀疏化参数，也就是卷积神经网络中所谓的下采样层。pooling 有两种，一种是最大值池化，一种是平均值池化，本例采用的是最大值池化tf.max_pool()。池化的核函数大小为2x2，因此ksize=[1,2,2,1]，步长为2，因此strides=[1,2,2,1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.029854",
     "start_time": "2017-02-27T13:46:43.024907"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_poo_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一次我们一层层的加上了不同的 layer. 分别是:\n",
    "- convolutional layer1 + max pooling;\n",
    "- convolutional layer2 + max pooling;\n",
    "- fully connected layer1 + dropout;\n",
    "- fully connected layer2 to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.043528",
     "start_time": "2017-02-27T13:46:43.033361"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = tf.placeholder(tf.float32, [None, 64])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还定义了dropout的placeholder，它是解决过拟合的有效手段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.051576",
     "start_time": "2017-02-27T13:46:43.046220"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着呢，我们需要处理我们的xs，把xs的形状变成[-1,8,8,1]，-1代表先不考虑输入的图片例子多少这个维度，后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1，例如如果是RGB图像，那么channel就是3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.065868",
     "start_time": "2017-02-27T13:46:43.054596"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(xs, [-1, 8, 8, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们定义第一层卷积,先定义本层的Weight,本层我们的卷积核patch的大小是5x5，因为黑白图片channel是1所以输入是1，输出是32个featuremap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.100477",
     "start_time": "2017-02-27T13:46:43.069539"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着定义bias，它的大小是32个长度，因此我们传入它的shape为[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.118518",
     "start_time": "2017-02-27T13:46:43.103199"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好了Weight和bias，我们就可以定义卷积神经网络的第一个卷积层h_conv1=conv2d(x_image,W_conv1)+b_conv1,同时我们对h_conv1进行非线性处理，也就是激活函数来处理喽，这里我们用的是tf.nn.relu（修正线性单元）来处理，要注意的是，因为采用了SAME的padding方式，输出图片的大小没有变化依然是8x8，只是厚度变厚了，因此现在的输出大小就变成了8x8x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.135543",
     "start_time": "2017-02-27T13:46:43.121757"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+b_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们再进行pooling的处理就ok啦，经过pooling的处理，输出大小就变为了7x7x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.146623",
     "start_time": "2017-02-27T13:46:43.138273"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_pool = max_poo_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着呢，同样的形式我们定义第二层卷积，本层我们的输入就是上一层的输出，本层我们的卷积核patch的大小是5x5，有32个featuremap所以输入就是32，输出呢我们定为64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.196054",
     "start_time": "2017-02-27T13:46:43.149379"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:22:21.681301",
     "start_time": "2017-02-27T13:22:21.676974"
    }
   },
   "source": [
    "接着我们就可以定义卷积神经网络的第二个卷积层，这时的输出的大小就是7x7x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.214968",
     "start_time": "2017-02-27T13:46:43.199189"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv2 = tf.nn.relu(conv2d(h_pool, W_conv2)+b_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后也是一个pooling处理，输出大小为3x3x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.239640",
     "start_time": "2017-02-27T13:46:43.217644"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_pool2 = max_poo_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，接下来我们定义我们的 fully connected layer,\n",
    "\n",
    "进入全连接层时, 我们通过tf.reshape()将h_pool2的输出值从一个三维的变为一维的数据, -1表示先不考虑输入图片例子维度, 将上一个输出结果展平."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.267271",
     "start_time": "2017-02-27T13:46:43.244152"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [n_samples, 3, 3, 64]-->>[n_samples, 3*3*64]\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 2*2*64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时weight_variable的shape输入就是第二个卷积层展平了的输出大小: 2x2x64， 后面的输出size为隐藏层神经元数目，我们继续扩大，定为256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:43.319872",
     "start_time": "2017-02-27T13:46:43.273080"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([2*2*64, 256])\n",
    "b_fc1 = bias_variable([256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将展平后的h_pool2_flat与本层的W_fc1相乘（注意这个时候不是卷积了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:46:49.429474",
     "start_time": "2017-02-27T13:46:49.416663"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们考虑过拟合问题，可以加一个dropout的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:48:15.766030",
     "start_time": "2017-02-27T13:48:15.744908"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们就可以进行最后一层的构建了，好激动啊, 输入是256，最后的输出是10个 (因为mnist数据集就是[0-9]十个类)，prediction就是我们最后的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:49:38.527226",
     "start_time": "2017-02-27T13:49:38.500789"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([256, 10])\n",
    "b_fc2 = bias_variable([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后呢我们用softmax分类器（多分类，输出是各个类的概率）,对我们的输出进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:51:46.059984",
     "start_time": "2017-02-27T13:51:46.049515"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着呢我们利用交叉熵损失函数来定义我们的cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:54:53.679576",
     "start_time": "2017-02-27T13:54:53.660480"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用tf.train.AdamOptimizer()作为我们的优化器进行优化，使我们的cross_entropy最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:55:52.914926",
     "start_time": "2017-02-27T13:55:52.446272"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义Session,初始化变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T13:58:15.333269",
     "start_time": "2017-02-27T13:58:14.996321"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好啦接着就是训练数据啦，我们假定训练1000步，每50步输出一下准确率， 注意sess.run()时记得要用feed_dict给我们的众多 placeholder 喂数据哦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T14:08:23.054216",
     "start_time": "2017-02-27T14:08:23.047739"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算精确度\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T14:16:25.020723",
     "start_time": "2017-02-27T14:10:58.966944"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125926\n",
      "0.138889\n",
      "0.157407\n",
      "0.331481\n",
      "0.52037\n",
      "0.674074\n",
      "0.733333\n",
      "0.746296\n",
      "0.772222\n",
      "0.787037\n",
      "0.82963\n",
      "0.844444\n",
      "0.853704\n",
      "0.866667\n",
      "0.874074\n",
      "0.890741\n",
      "0.903704\n",
      "0.912963\n",
      "0.92037\n",
      "0.925926\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train_step, feed_dict={xs: X_train, ys: y_train, keep_prob: 0.5})\n",
    "    if i % 50 == 0:\n",
    "        print compute_accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到准确率慢慢提升并到了92.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver 保存和读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存时, 首先要建立一个 tf.train.Saver() 用来保存, 提取变量. 再创建一个名为my_net的文件夹, 用这个 saver 来保存变量到这个目录 \"my_net/save_net.ckpt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T14:22:34.043790",
     "start_time": "2017-02-27T14:22:32.689046"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to path: my_net/save_net.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, 'my_net/save_net.ckpt')\n",
    "print 'save to path:', save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
